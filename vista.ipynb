{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["!pip install ultralytics"]},{"cell_type":"markdown","metadata":{},"source":["<span style=\"font-size:18px;\">Logarithmic transformation of dark images."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","import cv2\n","import numpy as np\n","\n","# Input and output directories\n","input_directory = \"./dataset/test\"\n","output_directory = \"./dataset2/test\"\n","\n","# Create the output directory if it doesn't exist\n","if not os.path.exists(output_directory):\n","    os.makedirs(output_directory)\n","\n","# Get all the image filenames in the input directory\n","image_names = [file for file in os.listdir(input_directory) if file.endswith(('.jpg', '.jpeg', '.png', '.gif'))]\n","\n","# Process each image\n","for name in image_names:\n","    # Read the image\n","    image_path = os.path.join(input_directory, name)\n","    gray_image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n","    \n","    average_pixel_value = np.mean(gray_image)\n","    if average_pixel_value<20:\n","        c = 255 / np.log(1 + np.max(gray_image))\n","        log_transformed_image = c * np.log(1 + gray_image)\n","\n","        # Clip the pixel values to ensure they are within the valid range [0, 255]\n","        log_transformed_image = np.clip(log_transformed_image, 0, 255).astype(np.uint8)\n","        final_image = np.stack((log_transformed_image,) * 3, axis=-1)\n","\n","    else:\n","        final_image = np.stack((gray_image,) * 3, axis=-1)\n","    \n","    # Save the transformed image to the output directory\n","    output_path = os.path.join(output_directory, name)\n","    cv2.imwrite(output_path, final_image)\n","\n","print(\"Log transformation completed and images saved to:\", output_directory)\n"]},{"cell_type":"markdown","metadata":{},"source":["> <span style=\"font-size:18px;\">Converting Bbox csv annotations to yolov8 annotations"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-28T15:20:37.648301Z","iopub.status.busy":"2024-03-28T15:20:37.646755Z","iopub.status.idle":"2024-03-28T15:23:07.737647Z","shell.execute_reply":"2024-03-28T15:23:07.735353Z","shell.execute_reply.started":"2024-03-28T15:20:37.648254Z"},"trusted":true},"outputs":[],"source":["#import os\n","import pandas as pd\n","import cv2 as cv\n","\n","# Function to convert annotations to YOLOv5 format\n","def convert_to_yolov8(annotation_file, image_folder, output_folder):\n","    df = pd.read_csv(annotation_file)\n","\n","    for index, row in df.iterrows():\n","        image_name = row['Name']\n","        class_name = row['class']\n","        xmin, ymin, xmax, ymax = row['xmin'], row['ymin'], row['xmax'], row['ymax']\n","\n","        image_path = os.path.join(image_folder, image_name)\n","        image_width, image_height = get_image_size(image_path)\n","\n","        x_center = (xmin + xmax) / (2 * image_width)\n","        y_center = (ymin + ymax) / (2 * image_height)\n","        width = (xmax - xmin) / image_width\n","        height = (ymax - ymin) / image_height\n","\n","        output_path = os.path.join(output_folder, image_name.replace('.jpg', '.txt'))\n","        with open(output_path, 'a') as out_f:\n","            out_f.write(f\"0 {x_center} {y_center} {width} {height}\\n\")\n","\n","# Function to get image size\n","def get_image_size(image_path):\n","    img=cv.imread(image_path)\n","    return img.shape[1],img.shape[0]\n","\n","\n","# Example usage\n","annotation_file = './dataset/bbox.csv'\n","image_folder = './dataset2/train'\n","output_folder = './dataset2/labels'\n","\n","# Create output folder if not exists\n","os.makedirs(output_folder, exist_ok=True)\n","\n","# Convert annotations\n","convert_to_yolov8(annotation_file, image_folder, output_folder)\n"]},{"cell_type":"markdown","metadata":{},"source":["<span style=\"font-size:18px;\">Dividing Training set into training and validation set(90-10 Division)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","import random\n","import shutil\n","\n","# Define paths to image dataset and labels folders\n","image_dataset_folder = './dataset2/train'\n","labels_folder = './dataset2/labels'\n","\n","# Define paths for training and validation folders\n","train_folder = './dataset3/train'\n","val_folder = './dataset3/val'\n","\n","# Create train and val folders if they don't exist\n","os.makedirs(train_folder, exist_ok=True)\n","os.makedirs(val_folder, exist_ok=True)\n","\n","# Create subdirectories for images and labels inside train and val folders\n","train_image_folder = os.path.join(train_folder, 'images')\n","train_label_folder = os.path.join(train_folder, 'labels')\n","val_image_folder = os.path.join(val_folder, 'images')\n","val_label_folder = os.path.join(val_folder, 'labels')\n","\n","os.makedirs(train_image_folder, exist_ok=True)\n","os.makedirs(train_label_folder, exist_ok=True)\n","os.makedirs(val_image_folder, exist_ok=True)\n","os.makedirs(val_label_folder, exist_ok=True)\n","\n","# Get list of image files\n","image_files = os.listdir(image_dataset_folder)\n","\n","# Get list of label files\n","label_files = os.listdir(labels_folder)\n","\n","# Extract filenames without extensions\n","image_filenames = {os.path.splitext(filename)[0] for filename in image_files}\n","label_filenames = {os.path.splitext(filename)[0] for filename in label_files}\n","\n","# Find common filenames between image and label datasets\n","common_filenames = image_filenames.intersection(label_filenames)\n","\n","# Shuffle the list of common filenames to ensure randomness\n","common_filenames = list(common_filenames)\n","random.shuffle(common_filenames)\n","\n","# Calculate split indices\n","split_index = int(0.9 * len(common_filenames))\n","\n","# Split the common filenames into training and validation sets\n","train_filenames = common_filenames[:split_index]\n","val_filenames = common_filenames[split_index:]\n","\n","# Copy images and labels to respective train and val folders\n","for filename in train_filenames:\n","    # Copy images\n","    shutil.copy(os.path.join(image_dataset_folder, filename + '.jpg'), os.path.join(train_image_folder, filename + '.jpg'))\n","    # Copy labels\n","    shutil.copy(os.path.join(labels_folder, filename + '.txt'), os.path.join(train_label_folder, filename + '.txt'))\n","\n","for filename in val_filenames:\n","    # Copy images\n","    shutil.copy(os.path.join(image_dataset_folder, filename + '.jpg'), os.path.join(val_image_folder, filename + '.jpg'))\n","    # Copy labels\n","    shutil.copy(os.path.join(labels_folder, filename + '.txt'), os.path.join(val_label_folder, filename + '.txt'))\n","\n","print(\"Dataset divided into training and validation sets successfully.\")\n"]},{"cell_type":"markdown","metadata":{},"source":["<span style=\"font-size:18px;\">Training process of the model"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-03-29T13:38:10.563500Z","iopub.status.busy":"2024-03-29T13:38:10.562852Z","iopub.status.idle":"2024-03-29T20:17:46.957712Z","shell.execute_reply":"2024-03-29T20:17:46.956118Z","shell.execute_reply.started":"2024-03-29T13:38:10.563468Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Ultralytics YOLOv8.1.37 🚀 Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\n","\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=/kaggle/working/last(2).pt, data=/kaggle/input/personcountvista/dataset3/data.yaml, epochs=100, time=None, patience=20, batch=10, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train2, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.0, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train2\n","Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 755k/755k [00:00<00:00, 42.8MB/s]\n","2024-03-29 13:38:14,423\tINFO util.py:124 -- Outdated packages:\n","  ipywidgets==7.7.1 found, needs ipywidgets>=8\n","Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n","2024-03-29 13:38:15,238\tINFO util.py:124 -- Outdated packages:\n","  ipywidgets==7.7.1 found, needs ipywidgets>=8\n","Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n","2024-03-29 13:38:17.265435: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-03-29 13:38:17.265560: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-03-29 13:38:17.373980: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train2', view at http://localhost:6006/\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":["  ········································\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/html":["wandb version 0.16.5 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.16.4"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20240329_133921-mzs28g2c</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/juzztjawa/YOLOv8/runs/mzs28g2c' target=\"_blank\">train2</a></strong> to <a href='https://wandb.ai/juzztjawa/YOLOv8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/juzztjawa/YOLOv8' target=\"_blank\">https://wandb.ai/juzztjawa/YOLOv8</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/juzztjawa/YOLOv8/runs/mzs28g2c' target=\"_blank\">https://wandb.ai/juzztjawa/YOLOv8/runs/mzs28g2c</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","                   from  n    params  module                                       arguments                     \n","  0                  -1  1      2320  ultralytics.nn.modules.conv.Conv             [3, 80, 3, 2]                 \n","  1                  -1  1    115520  ultralytics.nn.modules.conv.Conv             [80, 160, 3, 2]               \n","  2                  -1  3    436800  ultralytics.nn.modules.block.C2f             [160, 160, 3, True]           \n","  3                  -1  1    461440  ultralytics.nn.modules.conv.Conv             [160, 320, 3, 2]              \n","  4                  -1  6   3281920  ultralytics.nn.modules.block.C2f             [320, 320, 6, True]           \n","  5                  -1  1   1844480  ultralytics.nn.modules.conv.Conv             [320, 640, 3, 2]              \n","  6                  -1  6  13117440  ultralytics.nn.modules.block.C2f             [640, 640, 6, True]           \n","  7                  -1  1   3687680  ultralytics.nn.modules.conv.Conv             [640, 640, 3, 2]              \n","  8                  -1  3   6969600  ultralytics.nn.modules.block.C2f             [640, 640, 3, True]           \n","  9                  -1  1   1025920  ultralytics.nn.modules.block.SPPF            [640, 640, 5]                 \n"," 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n"," 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 12                  -1  3   7379200  ultralytics.nn.modules.block.C2f             [1280, 640, 3]                \n"," 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n"," 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 15                  -1  3   1948800  ultralytics.nn.modules.block.C2f             [960, 320, 3]                 \n"," 16                  -1  1    922240  ultralytics.nn.modules.conv.Conv             [320, 320, 3, 2]              \n"," 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 18                  -1  3   7174400  ultralytics.nn.modules.block.C2f             [960, 640, 3]                 \n"," 19                  -1  1   3687680  ultralytics.nn.modules.conv.Conv             [640, 640, 3, 2]              \n"," 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 21                  -1  3   7379200  ultralytics.nn.modules.block.C2f             [1280, 640, 3]                \n"," 22        [15, 18, 21]  1   8718931  ultralytics.nn.modules.head.Detect           [1, [320, 640, 640]]          \n","Model summary: 365 layers, 68153571 parameters, 68153555 gradients, 258.1 GFLOPs\n","\n","Transferred 595/595 items from pretrained weights\n","Freezing layer 'model.22.dfl.conv.weight'\n","\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n","Downloading https://github.com/ultralytics/assets/releases/download/v8.1.0/yolov8n.pt to 'yolov8n.pt'...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 6.23M/6.23M [00:00<00:00, 164MB/s]\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/personcountvista/dataset3/train/labels... 9209 images, 0 backgrounds, 0 corrupt: 100%|██████████| 9209/9209 [00:42<00:00, 216.01it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ Cache directory /kaggle/input/personcountvista/dataset3/train is not writeable, cache not saved.\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/personcountvista/dataset3/val/labels... 1024 images, 0 backgrounds, 0 corrupt: 100%|██████████| 1024/1024 [00:04<00:00, 224.55it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ Cache directory /kaggle/input/personcountvista/dataset3/val is not writeable, cache not saved.\n","Plotting labels to runs/detect/train2/labels.jpg... \n","\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01, momentum=0.9) with parameter groups 97 weight(decay=0.0), 104 weight(decay=0.00046875), 103 bias(decay=0.0)\n","Resuming training from /kaggle/working/last(2).pt from epoch 27 to 100 total epochs\n","\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ✅\n","Image sizes 640 train, 640 val\n","Using 4 dataloader workers\n","Logging results to \u001b[1mruns/detect/train2\u001b[0m\n","Starting training for 100 epochs...\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     27/100      12.4G       1.56     0.6446      1.379        104        640: 100%|██████████| 921/921 [17:30<00:00,  1.14s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 52/52 [00:27<00:00,  1.87it/s]\n"]},{"name":"stdout","output_type":"stream","text":["                   all       1024       6297      0.859      0.571      0.649      0.306\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     28/100      12.4G      1.559     0.6435       1.39        120        640: 100%|██████████| 921/921 [17:30<00:00,  1.14s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 52/52 [00:26<00:00,  1.95it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all       1024       6297      0.863      0.566      0.647      0.307\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     29/100      11.6G      1.547     0.6356      1.375         57        640: 100%|██████████| 921/921 [17:29<00:00,  1.14s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 52/52 [00:26<00:00,  1.94it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all       1024       6297      0.875      0.571      0.652      0.312\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     30/100      11.5G      1.531     0.6242      1.362         70        640: 100%|██████████| 921/921 [17:29<00:00,  1.14s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 52/52 [00:26<00:00,  1.94it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all       1024       6297      0.855      0.568      0.646      0.309\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     31/100      11.5G      1.517       0.62      1.347         79        640: 100%|██████████| 921/921 [17:29<00:00,  1.14s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 52/52 [00:26<00:00,  1.94it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all       1024       6297      0.845      0.579      0.648      0.304\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     32/100      11.5G      1.534     0.6314      1.358         93        640: 100%|██████████| 921/921 [17:29<00:00,  1.14s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 52/52 [00:26<00:00,  1.94it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all       1024       6297      0.865      0.578      0.652      0.312\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     33/100      11.5G      1.525     0.6233       1.34         84        640: 100%|██████████| 921/921 [17:29<00:00,  1.14s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 52/52 [00:26<00:00,  1.93it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all       1024       6297      0.868      0.574      0.652      0.311\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     34/100      11.5G      1.516     0.6209       1.34        121        640: 100%|██████████| 921/921 [17:29<00:00,  1.14s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 52/52 [00:26<00:00,  1.95it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all       1024       6297      0.852      0.574      0.653      0.307\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     35/100      11.5G      1.503     0.6104       1.35         67        640: 100%|██████████| 921/921 [17:30<00:00,  1.14s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 52/52 [00:26<00:00,  1.94it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all       1024       6297      0.863      0.574      0.653      0.311\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     36/100      11.5G      1.501     0.6108      1.339         98        640: 100%|██████████| 921/921 [17:30<00:00,  1.14s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 52/52 [00:26<00:00,  1.94it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all       1024       6297      0.866       0.58      0.655      0.309\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     37/100      11.5G      1.562     0.6405      1.367         79        640: 100%|██████████| 921/921 [17:30<00:00,  1.14s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 52/52 [00:26<00:00,  1.94it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all       1024       6297       0.87      0.574      0.653       0.31\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     38/100      11.5G      1.552     0.6303      1.355         84        640: 100%|██████████| 921/921 [17:29<00:00,  1.14s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 52/52 [00:26<00:00,  1.94it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all       1024       6297      0.863      0.579      0.655      0.314\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     39/100      11.5G      1.546     0.6265      1.351        106        640: 100%|██████████| 921/921 [17:29<00:00,  1.14s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 52/52 [00:26<00:00,  1.95it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all       1024       6297      0.866      0.576      0.655      0.314\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     40/100      11.5G      1.528     0.6223      1.351         78        640: 100%|██████████| 921/921 [17:29<00:00,  1.14s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 52/52 [00:26<00:00,  1.94it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all       1024       6297      0.853      0.581      0.654      0.314\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     41/100      11.5G      1.522     0.6173      1.356         89        640: 100%|██████████| 921/921 [17:29<00:00,  1.14s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 52/52 [00:26<00:00,  1.95it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all       1024       6297      0.853      0.583      0.654      0.312\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     42/100      11.5G      1.508     0.6139      1.339         74        640: 100%|██████████| 921/921 [17:29<00:00,  1.14s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 52/52 [00:26<00:00,  1.94it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all       1024       6297      0.861      0.583      0.661      0.316\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     43/100      11.5G        1.5     0.6038      1.335         82        640: 100%|██████████| 921/921 [17:29<00:00,  1.14s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 52/52 [00:26<00:00,  1.95it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all       1024       6297      0.847       0.59      0.663      0.316\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     44/100      11.5G      1.487     0.6004      1.336        105        640: 100%|██████████| 921/921 [17:29<00:00,  1.14s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 52/52 [00:26<00:00,  1.95it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all       1024       6297      0.847       0.59      0.664      0.316\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     45/100      11.6G      1.482     0.5991      1.334         74        640: 100%|██████████| 921/921 [17:29<00:00,  1.14s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 52/52 [00:26<00:00,  1.95it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all       1024       6297      0.859      0.585      0.662      0.314\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     46/100      11.5G      1.465     0.5944       1.32        103        640: 100%|██████████| 921/921 [17:30<00:00,  1.14s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 52/52 [00:26<00:00,  1.94it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all       1024       6297      0.843      0.594      0.666      0.314\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     47/100      11.5G      1.463     0.5938      1.295        151        640: 100%|██████████| 921/921 [17:30<00:00,  1.14s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 52/52 [00:26<00:00,  1.93it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all       1024       6297      0.857      0.589      0.663      0.315\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     48/100      11.6G      1.446     0.5848      1.291         59        640: 100%|██████████| 921/921 [17:29<00:00,  1.14s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 52/52 [00:26<00:00,  1.94it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all       1024       6297      0.873      0.578      0.662      0.315\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     49/100      11.5G      1.412     0.5781       1.28         92        640:   1%|▏         | 13/921 [00:15<17:52,  1.18s/it]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[8], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m YOLO(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/kaggle/working/last(2).pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# results = model.train(data='/kaggle/input/personcountvista/dataset3/data.yaml', epochs=100, patience=20,batch=-1)\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m results\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ultralytics/engine/model.py:667\u001b[0m, in \u001b[0;36mModel.train\u001b[0;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[1;32m    664\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mhub_session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession  \u001b[38;5;66;03m# attach optional HUB session\u001b[39;00m\n\u001b[0;32m--> 667\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ultralytics/engine/trainer.py:213\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    210\u001b[0m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 213\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ultralytics/engine/trainer.py:389\u001b[0m, in \u001b[0;36mBaseTrainer._do_train\u001b[0;34m(self, world_size)\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtloss \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    385\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtloss \u001b[38;5;241m*\u001b[39m i \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_items) \u001b[38;5;241m/\u001b[39m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_items\n\u001b[1;32m    386\u001b[0m     )\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# Backward\u001b[39;00m\n\u001b[0;32m--> 389\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;66;03m# Optimize - https://pytorch.org/docs/master/notes/amp_examples.html\u001b[39;00m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ni \u001b[38;5;241m-\u001b[39m last_opt_step \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccumulate:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["from ultralytics import YOLO\n","\n","model = YOLO('/kaggle/working/last(2).pt')\n","\n","# Train the model\n","# results = model.train(data='/kaggle/input/personcountvista/dataset3/data.yaml', epochs=100, patience=20,batch=-1)\n","results=model.train(resume=True)"]},{"cell_type":"markdown","metadata":{},"source":["Predicting test data values and storing it in CSV file"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-03-30T05:47:00.654412Z","iopub.status.busy":"2024-03-30T05:47:00.653488Z","iopub.status.idle":"2024-03-30T05:50:37.219539Z","shell.execute_reply":"2024-03-30T05:50:37.218772Z","shell.execute_reply.started":"2024-03-30T05:47:00.654375Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["CSV file has been created: outputfinal.csv\n"]}],"source":["from ultralytics import YOLO\n","import pandas as pd\n","from IPython.display import clear_output\n","import os\n","# Load a model\n","model = YOLO('/kaggle/working/best.pt')  # pretrained YOLOv8n model\n","directory = \"/kaggle/input/brightertest/test\"\n","\n","# Get all the filenames in the directory\n","image_names_relative = [file for file in os.listdir(directory)]\n","image_names=[\"/kaggle/input/wholetest/test/\"+x for x in image_names_relative]\n","# Run batched inference on a list of images\n","# results = model(['./dataset/train/100.jpg', './dataset/train/14170.jpg'])  # return a list of Results objects\n","final=[]\n","for i in range(len(image_names)):\n","    results=model.predict(image_names[i], save=True, conf=0.35,classes=[0],stream=True)\n","    results=list(results)\n","    final.append([image_names_relative[i],len(results[0])])\n","    clear_output()\n","del image_names\n","# print(len(results[0]))\n","# print(results[0].boxes)\n","df = pd.DataFrame(final, columns=['Name', 'HeadCount'])\n","\n","# Define the CSV file path\n","csv_file_path = \"outputfinal.csv\"\n","\n","# Write the DataFrame to a CSV file\n","df.to_csv(csv_file_path, index=False)\n","\n","print(\"CSV file has been created:\", csv_file_path)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4686515,"sourceId":7965591,"sourceType":"datasetVersion"},{"datasetId":4694837,"sourceId":7977320,"sourceType":"datasetVersion"},{"datasetId":4695940,"sourceId":7979019,"sourceType":"datasetVersion"},{"datasetId":4696059,"sourceId":7979207,"sourceType":"datasetVersion"},{"modelInstanceId":17543,"sourceId":21196,"sourceType":"modelInstanceVersion"},{"modelInstanceId":17674,"sourceId":21342,"sourceType":"modelInstanceVersion"},{"modelInstanceId":17725,"sourceId":21406,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30673,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
